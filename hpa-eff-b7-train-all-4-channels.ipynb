{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "leading-prediction",
   "metadata": {
    "papermill": {
     "duration": 0.009265,
     "end_time": "2021-04-23T16:14:37.120798",
     "exception": false,
     "start_time": "2021-04-23T16:14:37.111533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "waiting-nickname",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T16:14:37.151588Z",
     "iopub.status.busy": "2021-04-23T16:14:37.150845Z",
     "iopub.status.idle": "2021-04-23T16:14:54.351587Z",
     "shell.execute_reply": "2021-04-23T16:14:54.350898Z"
    },
    "papermill": {
     "duration": 17.222244,
     "end_time": "2021-04-23T16:14:54.351758",
     "exception": false,
     "start_time": "2021-04-23T16:14:37.129514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting focal-loss\r\n",
      "  Downloading focal_loss-0.0.6-py3-none-any.whl (18 kB)\r\n",
      "Requirement already satisfied: tensorflow>=2.2 in /opt/conda/lib/python3.7/site-packages (from focal-loss) (2.4.1)\r\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (3.7.4.3)\r\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (0.12.0)\r\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\r\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\r\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.19.5)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (3.15.8)\r\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (2.4.0)\r\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\r\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.12.1)\r\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (0.3.3)\r\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (2.4.1)\r\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.32.0)\r\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.12)\r\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\r\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (0.36.2)\r\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (2.10.0)\r\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (2.25.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (0.4.3)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.3.4)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.0.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.26.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (49.6.0.post20210108)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.8.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (4.7.2)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (4.2.1)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.4.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (0.4.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (2020.12.5)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (2.10)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.0.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.4.1)\r\n",
      "Installing collected packages: focal-loss\r\n",
      "Successfully installed focal-loss-0.0.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet -q\n",
    "!pip install focal-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floral-sarah",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T16:14:54.379879Z",
     "iopub.status.busy": "2021-04-23T16:14:54.379123Z",
     "iopub.status.idle": "2021-04-23T16:15:01.935552Z",
     "shell.execute_reply": "2021-04-23T16:15:01.934843Z"
    },
    "papermill": {
     "duration": 7.572704,
     "end_time": "2021-04-23T16:15:01.935700",
     "exception": false,
     "start_time": "2021-04-23T16:14:54.362996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from focal_loss import BinaryFocalLoss\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-howard",
   "metadata": {
    "papermill": {
     "duration": 0.011421,
     "end_time": "2021-04-23T16:15:01.958418",
     "exception": false,
     "start_time": "2021-04-23T16:15:01.946997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "breeding-wallpaper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T16:15:02.006315Z",
     "iopub.status.busy": "2021-04-23T16:15:02.005493Z",
     "iopub.status.idle": "2021-04-23T16:15:02.008883Z",
     "shell.execute_reply": "2021-04-23T16:15:02.008342Z"
    },
    "papermill": {
     "duration": 0.038832,
     "end_time": "2021-04-23T16:15:02.009028",
     "exception": false,
     "start_time": "2021-04-23T16:15:01.970196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binary_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss from the paper in multiclass classification\n",
    "    Formula:\n",
    "        loss = -alpha_t*((1-p_t)^gamma)*log(p_t)\n",
    "        \n",
    "        p_t = y_pred, if y_true = 1\n",
    "        p_t = 1-y_pred, otherwise\n",
    "        \n",
    "        alpha_t = alpha, if y_true=1\n",
    "        alpha_t = 1-alpha, otherwise\n",
    "        \n",
    "        cross_entropy = -log(p_t)\n",
    "    Parameters:\n",
    "        alpha -- the same as wighting factor in balanced cross entropy\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "        gamma -- 2.0 as mentioned in the paper\n",
    "        alpha -- 0.25 as mentioned in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    # Define epsilon so that the backpropagation will not result in NaN\n",
    "    # for 0 divisor case\n",
    "    epsilon = K.epsilon()\n",
    "    # Add the epsilon to prediction value\n",
    "    #y_pred = y_pred + epsilon\n",
    "    # Clip the prediciton value\n",
    "    y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "    # Calculate p_t\n",
    "    p_t = tf.where(K.equal(y_true, 1), y_pred, 1-y_pred)\n",
    "    # Calculate alpha_t\n",
    "    alpha_factor = K.ones_like(y_true)*alpha\n",
    "    alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1-alpha_factor)\n",
    "    # Calculate cross entropy\n",
    "    cross_entropy = -K.log(p_t)\n",
    "    weight = alpha_t * K.pow((1-p_t), gamma)\n",
    "    # Calculate focal loss\n",
    "    loss = weight * cross_entropy\n",
    "    # Sum the losses in mini_batch\n",
    "    loss = K.sum(loss, axis=1)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def auto_select_accelerator():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    \n",
    "    return strategy\n",
    "\n",
    "\n",
    "def build_decoder(with_labels=True, target_size=(256, 256), ext='jpg'):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels=3)\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        img = tf.image.resize(img, target_size)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return decode(path), label\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode\n",
    "\n",
    "\n",
    "def build_augmenter(with_labels=True):\n",
    "    def augment(img):\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):\n",
    "        return augment(img), label\n",
    "    \n",
    "    return augment_with_labels if with_labels else augment\n",
    "\n",
    "\n",
    "def build_dataset(paths, labels=None, bsize=128, cache=True,\n",
    "                  decode_fn=None, augment_fn=None,\n",
    "                  augment=True, repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\"):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter(labels is not None)\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = paths if labels is None else (paths, labels)\n",
    "    \n",
    "    dset = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    dset = dset.cache(cache_dir) if cache else dset\n",
    "    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n",
    "    dset = dset.repeat() if repeat else dset\n",
    "    dset = dset.shuffle(shuffle) if shuffle else dset\n",
    "    dset = dset.batch(bsize).prefetch(AUTO)\n",
    "    \n",
    "    return dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-trading",
   "metadata": {
    "papermill": {
     "duration": 0.012595,
     "end_time": "2021-04-23T16:15:02.032736",
     "exception": false,
     "start_time": "2021-04-23T16:15:02.020141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selected-ceremony",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T16:15:02.064773Z",
     "iopub.status.busy": "2021-04-23T16:15:02.064071Z",
     "iopub.status.idle": "2021-04-23T16:15:08.737348Z",
     "shell.execute_reply": "2021-04-23T16:15:08.736530Z"
    },
    "papermill": {
     "duration": 6.693751,
     "end_time": "2021-04-23T16:15:08.737566",
     "exception": false,
     "start_time": "2021-04-23T16:15:02.043815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU: grpc://10.0.0.2:8470\n",
      "Running on 8 replicas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://kds-b685f8a833c7d14d373d0373cb155e4ef69f97f30d2907e6adfbd2a1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPETITION_NAME = \"hpa-768768\"\n",
    "strategy = auto_select_accelerator()\n",
    "BATCH_SIZE = strategy.num_replicas_in_sync * 16\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path(COMPETITION_NAME)\n",
    "GCS_DS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifth-phase",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T16:15:08.773759Z",
     "iopub.status.busy": "2021-04-23T16:15:08.773017Z",
     "iopub.status.idle": "2021-04-23T16:15:53.898349Z",
     "shell.execute_reply": "2021-04-23T16:15:53.898873Z"
    },
    "papermill": {
     "duration": 45.148871,
     "end_time": "2021-04-23T16:15:53.899043",
     "exception": false,
     "start_time": "2021-04-23T16:15:08.750172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "258441216/258434480 [==============================] - 5s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b7 (Functional) (None, 25, 25, 2560)      64097680  \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               655616    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 19)                1235      \n",
      "=================================================================\n",
      "Total params: 64,770,979\n",
      "Trainable params: 64,460,259\n",
      "Non-trainable params: 310,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMSIZE = (224, 240, 260, 300, 380, 456, 528, 600, 700, 800)\n",
    "IMS = 9\n",
    "n_labels = 19 \n",
    "binary_focal_loss = BinaryFocalLoss(gamma=5)\n",
    "\n",
    "decoder = build_decoder(with_labels=True, target_size=(IMSIZE[IMS], IMSIZE[IMS]))\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        efn.EfficientNetB7(\n",
    "            input_shape=(IMSIZE[IMS], IMSIZE[IMS], 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False),\n",
    "        #tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        # tf.keras.layers.Dense(n_labels, activation='sigmoid')\n",
    "        tf.keras.layers.GlobalMaxPool2D(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(n_labels, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adadelta(), # optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=binary_focal_loss, # 'binary_crossentropy'\n",
    "        metrics=[tf.keras.metrics.AUC(multi_label=True)])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-buddy",
   "metadata": {
    "papermill": {
     "duration": 0.038751,
     "end_time": "2021-04-23T16:15:53.976547",
     "exception": false,
     "start_time": "2021-04-23T16:15:53.937796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "official-simple",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T16:15:54.058812Z",
     "iopub.status.busy": "2021-04-23T16:15:54.058128Z",
     "iopub.status.idle": "2021-04-23T16:18:42.253211Z",
     "shell.execute_reply": "2021-04-23T16:18:42.254154Z"
    },
    "papermill": {
     "duration": 168.238123,
     "end_time": "2021-04-23T16:18:42.254379",
     "exception": false,
     "start_time": "2021-04-23T16:15:54.016256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "9 root error(s) found.\n  (0) Resource exhausted: {{function_node __inference_train_function_163742}} Compilation failure: Ran out of memory in memory space hbm. Used 25.74G of 15.48G hbm. Exceeded hbm capacity by 10.26G.\n\nTotal hbm usage >= 26.26G:\n    reserved        530.00M \n    program          25.74G \n    arguments       unknown size \n\nOutput size unknown.\n\nProgram hbm requirement 25.74G:\n    global            84.0K\n    scoped           68.92M\n    HLO temp         25.68G (77.0% utilization: Unpadded (18.42G) Padded (23.92G), 6.8% fragmentation (1.75G))\n\n  Largest program allocations in hbm:\n\n  1. Size: 937.50M\n     Operator: op_type=\"DepthwiseConv2dNativeBackpropInput\" op_name=\"gradient_tape/sequential/efficientnet-b7/block2b_dwconv/depthwise/DepthwiseConv2dNativeBackpropInput\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.2580 = (f32[288]{0:T(512)}, f32[288]{0:T(512)}, f32[16,200,200,288]{3,0,2,1:T(8,128)}) fusion(s32[]{:T(256)} %get-tuple-element.40607, f32[16,200,200,288]{3,0,2,1:T(8,128)} %fusion.3652.remat2, f32[288]{0:T(512)} %get-tuple-element.39220, f32[288]{...\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 937.50M\n     Operator: op_type=\"DepthwiseConv2dNativeBackpropInput\" op_name=\"gradient_tape/sequential/efficientnet-b7/block2d_dwconv/depthwise/DepthwiseConv2dNativeBackpropInput\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.2570 = (f32[288]{0:T(512)}, f32[288]{0:T(512)}, f32[16,200,200,288]{3,0,2,1:T(8,128)}) fusion(s32[]{:T(256)} %get-tuple-element.40607, f32[16,200,200,288]{3,0,2,1:T(8,128)} %fusion.22802.remat5, f32[288]{0:T(512)} %get-tuple-element.39136, f32[288]...\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 937.50M\n     Operator: op_type=\"Conv2D\" op_name=\"sequential/efficientnet-b7/block2b_expand_conv/Conv2D\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.3652.remat2 = f32[16,200,200,288]{3,0,2,1:T(8,128)} fusion(f32[1,1,48,288]{3,2,1,0:T(8,128)} %get-tuple-element.42138, f32[48]{0:T(256)} %fusion.22085, f32[48]{0:T(256)} %get-tuple-element.42123, f32[48]{0:T(256)} %get-tuple-element.42122, f32[16,2...\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 937.50M\n     Operator: op_type=\"DepthwiseConv2dNativeBackpropInput\" op_name=\"gradient_tape/sequential/efficientnet-b7/block2c_dwconv/depthwise/DepthwiseConv2dNativeBackpropInput\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.2575 = (f32[288]{0:T(512)}, f32[288]{0:T(512)}, f32[16,200,200,288]{3,0,2,1:T(8,128)}) fusion(s32[]{:T(256)} %get-tuple-element.40607, f32[16,200,200,288]{3,0,2,1:T(8,128)} %convolution.2748.r ... [truncated]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0880e6d36308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_reducer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         validation_data=valid_dataset)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mhist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1037\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 9 root error(s) found.\n  (0) Resource exhausted: {{function_node __inference_train_function_163742}} Compilation failure: Ran out of memory in memory space hbm. Used 25.74G of 15.48G hbm. Exceeded hbm capacity by 10.26G.\n\nTotal hbm usage >= 26.26G:\n    reserved        530.00M \n    program          25.74G \n    arguments       unknown size \n\nOutput size unknown.\n\nProgram hbm requirement 25.74G:\n    global            84.0K\n    scoped           68.92M\n    HLO temp         25.68G (77.0% utilization: Unpadded (18.42G) Padded (23.92G), 6.8% fragmentation (1.75G))\n\n  Largest program allocations in hbm:\n\n  1. Size: 937.50M\n     Operator: op_type=\"DepthwiseConv2dNativeBackpropInput\" op_name=\"gradient_tape/sequential/efficientnet-b7/block2b_dwconv/depthwise/DepthwiseConv2dNativeBackpropInput\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.2580 = (f32[288]{0:T(512)}, f32[288]{0:T(512)}, f32[16,200,200,288]{3,0,2,1:T(8,128)}) fusion(s32[]{:T(256)} %get-tuple-element.40607, f32[16,200,200,288]{3,0,2,1:T(8,128)} %fusion.3652.remat2, f32[288]{0:T(512)} %get-tuple-element.39220, f32[288]{...\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 937.50M\n     Operator: op_type=\"DepthwiseConv2dNativeBackpropInput\" op_name=\"gradient_tape/sequential/efficientnet-b7/block2d_dwconv/depthwise/DepthwiseConv2dNativeBackpropInput\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.2570 = (f32[288]{0:T(512)}, f32[288]{0:T(512)}, f32[16,200,200,288]{3,0,2,1:T(8,128)}) fusion(s32[]{:T(256)} %get-tuple-element.40607, f32[16,200,200,288]{3,0,2,1:T(8,128)} %fusion.22802.remat5, f32[288]{0:T(512)} %get-tuple-element.39136, f32[288]...\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 937.50M\n     Operator: op_type=\"Conv2D\" op_name=\"sequential/efficientnet-b7/block2b_expand_conv/Conv2D\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.3652.remat2 = f32[16,200,200,288]{3,0,2,1:T(8,128)} fusion(f32[1,1,48,288]{3,2,1,0:T(8,128)} %get-tuple-element.42138, f32[48]{0:T(256)} %fusion.22085, f32[48]{0:T(256)} %get-tuple-element.42123, f32[48]{0:T(256)} %get-tuple-element.42122, f32[16,2...\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 937.50M\n     Operator: op_type=\"DepthwiseConv2dNativeBackpropInput\" op_name=\"gradient_tape/sequential/efficientnet-b7/block2c_dwconv/depthwise/DepthwiseConv2dNativeBackpropInput\"\n     Shape: f32[16,200,200,288]{3,0,2,1:T(8,128)}\n     Unpadded size: 703.12M\n     Extra memory due to padding: 234.38M (1.3x expansion)\n     XLA label: %fusion.2575 = (f32[288]{0:T(512)}, f32[288]{0:T(512)}, f32[16,200,200,288]{3,0,2,1:T(8,128)}) fusion(s32[]{:T(256)} %get-tuple-element.40607, f32[16,200,200,288]{3,0,2,1:T(8,128)} %convolution.2748.r ... [truncated]"
     ]
    }
   ],
   "source": [
    "# color data generating\n",
    "load_dir = f\"/kaggle/input/{COMPETITION_NAME}/\"\n",
    "df_in = pd.read_csv('../input/hpa-labels-csv-hg/df_green.csv')\n",
    "\n",
    "colours = ['green', 'blue', 'red', 'yellow']\n",
    "for colour in colours:\n",
    "    df = df_in.copy()\n",
    "    df.ID = df.ID[0][:-5] + colour\n",
    "\n",
    "    label_cols = df.columns[2:21]\n",
    "    paths = GCS_DS_PATH + '/' + df['ID'] + '.png'\n",
    "    labels = df[label_cols].values\n",
    "\n",
    "    (train_paths, valid_paths, train_labels, valid_labels) = train_test_split(paths, labels, test_size=0.2, random_state=42)\n",
    "    train_dataset = build_dataset(train_paths, train_labels, bsize=BATCH_SIZE, decode_fn=decoder)\n",
    "    valid_dataset = build_dataset(valid_paths, valid_labels, bsize=BATCH_SIZE, decode_fn=decoder,\n",
    "                                  repeat=False, shuffle=False, augment=False)\n",
    "    \n",
    "    steps_per_epoch = train_paths.shape[0] // BATCH_SIZE\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'effb7_individual_model_{colour}_800.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", patience=3, min_lr=1e-6, mode='min')\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset, \n",
    "        epochs=20,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint, lr_reducer],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=valid_dataset)\n",
    "\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f'effb7_individual_history_{colour}_800.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-perth",
   "metadata": {
    "papermill": {
     "duration": 0.040305,
     "end_time": "2021-04-23T16:18:42.335121",
     "exception": false,
     "start_time": "2021-04-23T16:18:42.294816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "- [[HPA] classification efnb7 train](https://www.kaggle.com/h053473666/hpa-classification-efnb7-train)\n",
    "- [[HPA] classification efnb7 train 13cc0d](https://www.kaggle.com/aristotelisch/hpa-classification-efnb7-train-13cc0d?scriptVersionId=60520853)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 256.931785,
   "end_time": "2021-04-23T16:18:45.791984",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-23T16:14:28.860199",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
